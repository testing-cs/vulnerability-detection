import json
import pdb
import sys
import torch
import argparse

sys.path.append('../')
sys.path.append('python_parser')

from python_parser.run_parser import get_identifiers, remove_comments_and_docstrings, get_example
from utils import is_valid_variable_name, _tokenize, get_identifier_posistions_from_code, get_substitues, is_valid_substitue
from transformers import (RobertaForMaskedLM, RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--data_root", default="lin2018", type=str,
                        help="The root path of the dataset.")
    parser.add_argument("--project_name", default="LibTIFF", type=str,
                        help="The name of the project.")
    parser.add_argument("--base_model", default="microsoft/graphcodebert-base-mlm", type=str,
                        help="Base Model")
    parser.add_argument("--block_size", default=400, type=int,
                        help="Optional input sequence length after tokenization.")
    args = parser.parse_args()
    args.train_data_file = f'../{args.data_root}/{args.project_name}_train.jsonl'

    codebert_mlm = RobertaForMaskedLM.from_pretrained(args.base_model)
    tokenizer_mlm = RobertaTokenizer.from_pretrained(args.base_model)
    codebert_mlm.to(device)

    sec_count = 0
    vul_count = 0
    vul_data = []
    with open(args.train_data_file) as rf:
        for i, line in enumerate(rf):
            item = json.loads(line.strip())
            if item['target'] == 0:
                sec_count += 1
            else:
                vul_count += 1
                vul_data.append(item)
    print(f"sec: {sec_count}, vul: {vul_count}")

    index = 0
    with open(f"../{args.data_root}/{args.project_name}_train_a.jsonl", "w") as wf:
        for item in vul_data:
            identifiers, code_tokens = get_identifiers(remove_comments_and_docstrings(item["func"], "c"), "c")
            processed_code = " ".join(code_tokens)
            words, sub_words, keys = _tokenize(processed_code, tokenizer_mlm)
            variable_names = []
            for name in identifiers:
                if ' ' in name[0].strip():
                    continue
                variable_names.append(name[0])
            sub_words = [tokenizer_mlm.cls_token] + sub_words[:args.block_size - 2] + [tokenizer_mlm.sep_token]
            input_ids_ = torch.tensor([tokenizer_mlm.convert_tokens_to_ids(sub_words)])
            word_predictions = codebert_mlm(input_ids_.to(device))[0].squeeze()  # seq-len(sub) vocab
            _, word_predictions = torch.topk(word_predictions, 100, -1)  # seq-len k

            word_predictions = word_predictions[1:len(sub_words) + 1, :]
            names_positions_dict = get_identifier_posistions_from_code(words, variable_names)

            for tgt_word in names_positions_dict.keys():
                variable_substitue_dict = []
                tgt_positions = names_positions_dict[tgt_word]  # the positions of tgt_word in code
                all_substitues = []
                for one_pos in tgt_positions:
                    if keys[one_pos][0] >= word_predictions.size()[0]:
                        continue
                substitutes = word_predictions[keys[one_pos][0]:keys[one_pos][1]]  # L, k
                substitutes = get_substitues(substitutes, tokenizer_mlm, codebert_mlm)
                all_substitues += substitutes
                all_substitues = set(all_substitues)
                for tmp_substitue in all_substitues:
                    if tmp_substitue.strip() in variable_names:
                        continue
                    if not is_valid_substitue(tmp_substitue.strip(), tgt_word, 'c'):
                        continue
                    try:
                        variable_substitue_dict.append(tmp_substitue)
                    except:
                        variable_substitue_dict = [tmp_substitue]
                for substitute in variable_substitue_dict:
                    new_code = get_example(item['func'], tgt_word, substitute, "c")
                    try:
                        js = {"project": item["project"], "target": 1, "func": new_code, "bug_type": item["bug_type"], "idx": index}
                    except:
                        js = {"project": item["project"], "target": 1, "func": new_code, "idx": index}
                    wf.write(json.dumps(js) + '\n')
                    index += 1
    print(index)


if __name__ == "__main__":
    main()
